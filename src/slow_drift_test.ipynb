{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff50bd12-985e-4747-af6c-9a4e5125dffb",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0c4215-7654-4dbf-a8dc-5ea977f2140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working dir:  /u/csanthirasegaran/IBL_foundation_model\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# set your working dir\n",
    "work_dir = '/u/csanthirasegaran/IBL_foundation_model'\n",
    "os.chdir(work_dir)\n",
    "print('working dir: ', work_dir)\n",
    "\n",
    "path = 'src'\n",
    "sys.path.append(str(path))\n",
    "\n",
    "from src.loader.make_loader import make_loader\n",
    "from src.utils.dataset_utils import load_ibl_dataset\n",
    "from src.utils.utils import set_seed, move_batch_to_device, plot_gt_pred, metrics_list, plot_avg_rate_and_spike, \\\n",
    "    plot_rate_and_spike\n",
    "from src.utils.config_utils import config_from_kwargs, update_config\n",
    "from src.models.stpatch import STPatch\n",
    "from src.trainer.make import make_trainer\n",
    "from src.utils.eval_utils import (\n",
    "    heldout_mask, \n",
    "    neg_log_likelihood,\n",
    "    # create_behave_list,\n",
    "    bits_per_spike,\n",
    "    plot_psth,\n",
    "    plot_single_trial_activity,\n",
    "    viz_single_cell,\n",
    "    viz_single_cell_unaligned,\n",
    "    _add_baseline,\n",
    "    raster_plot,\n",
    "    compute_PSTH,\n",
    "    compute_all_psth,\n",
    "    compute_R2_psth,\n",
    "    compute_R2_main\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5a91fc-34fb-4bde-be5d-d1de09795fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from math import ceil\n",
    "from accelerate import Accelerator\n",
    "from loader.base import BaseDataset\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets, DatasetDict\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.special import gammaln\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import matplotlib.colors as colors\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from models.ndt1 import NDT1\n",
    "from src.models.neurotoken import Neurotokenizer\n",
    "\n",
    "import logging\n",
    "\n",
    "NAME2MODEL = {\"STPatch\": STPatch, \"NDT1\": NDT1, \"Neurotoken\": Neurotokenizer}\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f7846-748a-43f4-8d75-3ddd3e897f96",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d3d3f3-93a2-4475-845c-7edb9c1ce6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# Model/Dataset Loading and Configuration\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def load_model_data_local(**kwargs):\n",
    "    model_config = kwargs['model_config']\n",
    "    trainer_config = kwargs['trainer_config']\n",
    "    model_path = kwargs['model_path']\n",
    "    dataset_path = kwargs['dataset_path']\n",
    "    test_size = kwargs['test_size']\n",
    "    seed = kwargs['seed']\n",
    "    mask_name = kwargs['mask_name']\n",
    "    mask_mode = mask_name.split(\"_\")[1]\n",
    "    eid = kwargs['eid']\n",
    "    stitching = kwargs['stitching']\n",
    "    num_sessions = kwargs['num_sessions']\n",
    "    tokenize_binary_mask = kwargs['tokenize_binary_mask']\n",
    "\n",
    "    # set seed\n",
    "    set_seed(seed)\n",
    "\n",
    "    # load the model\n",
    "    config = config_from_kwargs({\"model\": f\"include:{model_config}\"})\n",
    "    config = update_config(model_config, config)\n",
    "    config = update_config(trainer_config, config)\n",
    "    config.model.encoder.masker.mode = mask_mode\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset, meta_data = load_ibl_dataset(\n",
    "                            cache_dir=config.dirs.dataset_cache_dir,\n",
    "                            user_or_org_name=\"ibl-foundation-model\",\n",
    "                            num_sessions=config.data.num_sessions,\n",
    "                            split_method=config.data.split_method,\n",
    "                            train_session_eid=[eid],\n",
    "                            test_session_eid=config.data.test_session_eid,\n",
    "                            batch_size=config.training.train_batch_size,\n",
    "                            seed=seed,\n",
    "                            eid=None,\n",
    "                        )\n",
    "    \n",
    "    meta_data[\"tokenize_binary_mask\"] = True if tokenize_binary_mask else False\n",
    "\n",
    "    # load the dataset\n",
    "    dataset = load_dataset(f'ibl-foundation-model/{eid}_aligned', cache_dir=config.dirs.dataset_cache_dir)[\"test\"]\n",
    "\n",
    "    n_neurons = len(dataset['cluster_regions'][0])\n",
    "\n",
    "    if config.model.model_class in [\"NDT1\", \"iTransformer\"]:\n",
    "        max_space_length = n_neurons  \n",
    "    elif config.model.model_class in [\"NDT2\", \"STPatch\"]:\n",
    "        # max_space_length = config.model.encoder.embedder.n_neurons\n",
    "        max_space_F = config.model.encoder.embedder.max_space_F\n",
    "        max_num_neurons = max(meta_data['num_neurons'])\n",
    "        max_space_length = ceil(max_num_neurons/max_space_F) * max_space_F\n",
    "    else:\n",
    "        max_space_length = config.data.max_space_length\n",
    "\n",
    "    meta_data['max_space_length'] = max_space_length\n",
    "\n",
    "    print('encoder max space length:', max_space_length)\n",
    "    \n",
    "    print(meta_data)\n",
    "\n",
    "    model_class = NAME2MODEL[config.model.model_class]\n",
    "    model = model_class(config.model, **config.method.model_kwargs, **meta_data)    \n",
    "    model = torch.load(model_path, map_location=torch.device('cpu'))['model']\n",
    "\n",
    "    model.encoder.masker.mode = mask_mode\n",
    "    model.encoder.masker.force_active = False\n",
    "\n",
    "    print(\"(eval) masking mode: \", model.encoder.masker.mode)\n",
    "    print(\"(eval) masking ratio: \", model.encoder.masker.ratio)\n",
    "    print(\"(eval) masking active: \", model.encoder.masker.force_active)\n",
    "    if 'causal' in mask_name:\n",
    "        model.encoder.context_forward = 0\n",
    "        print(\"(behave decoding) context forward: \", model.encoder.context_forward)\n",
    "    \n",
    "    model = accelerator.prepare(model)\n",
    "\n",
    "    dataloader = make_loader(\n",
    "        dataset,\n",
    "        target=config.data.target,\n",
    "        batch_size=len(dataset),\n",
    "        pad_to_right=True,\n",
    "        pad_value=-1.,\n",
    "        max_time_length=config.data.max_time_length,\n",
    "        max_space_length=max_space_length,\n",
    "        dataset_name=config.data.dataset_name,\n",
    "        load_meta=config.data.load_meta,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # check the shape of the dataset\n",
    "    for batch in dataloader:\n",
    "        print('spike data shape: {}'.format(batch['spikes_data'].shape))\n",
    "        break\n",
    "\n",
    "    return model, accelerator, dataset, dataloader\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# Evaluation\n",
    "# 1. Co-smoothing_eval (R2, co-bps, and shuqi's plot) \n",
    "# 2. Behavior_decoding (choice, wheel speed)\n",
    "# 3. R2 scatter plot\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "def co_smoothing_eval(\n",
    "        model,\n",
    "        accelerator,\n",
    "        test_dataloader,\n",
    "        test_dataset,\n",
    "        n=1,\n",
    "        save_plot=False,\n",
    "        **kwargs\n",
    "):\n",
    "    assert n == 1, 'only support n=1 now'\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        break\n",
    "\n",
    "    method_name = kwargs['method_name']\n",
    "    mode = kwargs['mode']\n",
    "    is_aligned = kwargs['is_aligned']\n",
    "    target_regions = kwargs['target_regions']\n",
    "    n_jobs = kwargs['n_jobs']\n",
    "\n",
    "    # hack to accommodate NDT2 - fix later \n",
    "    if sum(batch['space_attn_mask'][0] == 0) == 0:\n",
    "        tot_num_neurons = batch['space_attn_mask'].size()[-1]\n",
    "    else:\n",
    "        tot_num_neurons = (batch['space_attn_mask'][0] == 0).nonzero().min().item() \n",
    "    uuids_list = np.array(test_dataset['cluster_uuids'][0])[:tot_num_neurons]\n",
    "    region_list = np.array(test_dataset['cluster_regions'])[0][:tot_num_neurons]\n",
    "\n",
    "    T = kwargs['n_time_steps']\n",
    "    N = uuids_list.shape[0]    \n",
    "\n",
    "    if is_aligned:\n",
    "        \n",
    "        # prepare the condition matrix\n",
    "        b_list = []\n",
    "    \n",
    "        # choice\n",
    "        choice = np.array(test_dataset['choice'])\n",
    "        choice = np.tile(np.reshape(choice, (choice.shape[0], 1)), (1, T))\n",
    "        b_list.append(choice)\n",
    "    \n",
    "        # reward\n",
    "        reward = np.array(test_dataset['reward'])\n",
    "        reward = np.tile(np.reshape(reward, (reward.shape[0], 1)), (1, T))\n",
    "        b_list.append(reward)\n",
    "    \n",
    "        # block\n",
    "        block = np.array(test_dataset['block'])\n",
    "        block = np.tile(np.reshape(block, (block.shape[0], 1)), (1, T))\n",
    "        b_list.append(block)\n",
    "    \n",
    "        behavior_set = np.stack(b_list, axis=-1)\n",
    "    \n",
    "        var_name2idx = {'block': [2],\n",
    "                        'choice': [0],\n",
    "                        'reward': [1],\n",
    "                        'wheel': [3],\n",
    "                        }\n",
    "        var_value2label = {'block': {(0.2,): \"p(left)=0.2\",\n",
    "                                     (0.5,): \"p(left)=0.5\",\n",
    "                                     (0.8,): \"p(left)=0.8\", },\n",
    "                           'choice': {(-1.0,): \"right\",\n",
    "                                      (1.0,): \"left\"},\n",
    "                           'reward': {(0.,): \"no reward\",\n",
    "                                      (1.,): \"reward\", }}\n",
    "        var_tasklist = ['block', 'choice', 'reward']\n",
    "        var_behlist = []\n",
    "\n",
    "    if mode == 'per_neuron':\n",
    "\n",
    "        gt_result_list, pred_result_list = [], []\n",
    "        bps_result_list, r2_result_list = [float('nan')] * tot_num_neurons, [np.array([np.nan, np.nan])] * N\n",
    "        # loop through all the neurons\n",
    "        # for n_i in tqdm(range(tot_num_neurons)):\n",
    "        bps_per_region = []; region = 'CA1'\n",
    "        for n_i in tqdm(np.argwhere(region_list == region).flatten()):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in test_dataloader:\n",
    "                    batch = move_batch_to_device(batch, accelerator.device)     \n",
    "                        \n",
    "                    mask_result = heldout_mask(\n",
    "                        batch['spikes_data'].clone(),\n",
    "                        mode='manual',\n",
    "                        heldout_idxs=np.array([n_i])\n",
    "                    )\n",
    "                    try:\n",
    "                        masking_mode = 'neuron' if model.use_prompt else model.encoder.masker.mode\n",
    "                        model.encoder.mask = False\n",
    "                    except AttributeError:\n",
    "                        masking_mode = 'neuron' if model.use_prompt else model.masker.mode\n",
    "                        model.mask = False\n",
    "                    \n",
    "                    outputs = model(\n",
    "                        mask_result['spikes'],\n",
    "                        time_attn_mask=batch['time_attn_mask'],\n",
    "                        space_attn_mask=batch['space_attn_mask'],\n",
    "                        spikes_timestamps=batch['spikes_timestamps'], \n",
    "                        spikes_spacestamps=batch['spikes_spacestamps'], \n",
    "                        targets = batch['target'],\n",
    "                        neuron_regions=batch['neuron_regions'],\n",
    "                        eval_mask=mask_result['eval_mask'],\n",
    "                        masking_mode = masking_mode,\n",
    "                        num_neuron=batch['spikes_data'].shape[2],\n",
    "                        eid=batch['eid'][0],\n",
    "                    )\n",
    "            outputs.preds = torch.exp(outputs.preds)\n",
    "    \n",
    "            gt_spikes = batch['spikes_data'].detach().cpu().numpy()\n",
    "            pred_spikes = outputs.preds.detach().cpu().numpy()\n",
    "\n",
    "            # compute co-bps\n",
    "            gt_held_out = gt_spikes[:, :, [n_i]]\n",
    "            pred_held_out = pred_spikes[:, :, [n_i]]\n",
    "\n",
    "            bps = bits_per_spike(pred_held_out, gt_held_out)\n",
    "            if np.isinf(bps):\n",
    "                bps = np.nan\n",
    "            bps_result_list[n_i] = bps\n",
    "            bps_per_region.append(bps)\n",
    "\n",
    "            gt_result_list.append(gt_held_out)\n",
    "            pred_result_list.append(pred_held_out)\n",
    "\n",
    "            # compute R2\n",
    "            if is_aligned:\n",
    "                X = behavior_set  # [#trials, #timesteps, #variables]\n",
    "                _r2_psth, _r2_trial = viz_single_cell(X, gt_held_out.squeeze(), pred_held_out.squeeze(),\n",
    "                                                      var_name2idx, var_tasklist, var_value2label, var_behlist,\n",
    "                                                      subtract_psth=kwargs['subtract'],\n",
    "                                                      aligned_tbins=kwargs['onset_alignment'],\n",
    "                                                      neuron_idx=uuids_list[n_i][:4],\n",
    "                                                      neuron_region=region_list[n_i],\n",
    "                                                      method=method_name, save_path=kwargs['save_path'],\n",
    "                                                      save_plot=save_plot\n",
    "                                                     )\n",
    "                r2_result_list[n_i] = np.array([_r2_psth, _r2_trial])\n",
    "            else:\n",
    "                r2 = viz_single_cell_unaligned(\n",
    "                    gt_held_out.squeeze(), pred_held_out.squeeze(), \n",
    "                    neuron_idx=uuids_list[n_i][:4],\n",
    "                    neuron_region=region_list[n_i],\n",
    "                    method=method_name, save_path=kwargs['save_path'],\n",
    "                    save_plot=save_plot\n",
    "                )\n",
    "                r2_result_list[n_i] = r2\n",
    "                \n",
    "        print(f'{region} bps: ', np.nanmean(bps_per_region))\n",
    "\n",
    "    elif mode == 'inter_region':\n",
    "\n",
    "        if 'all' in target_regions:\n",
    "            target_regions = list(np.unique(region_list))\n",
    "            \n",
    "        held_out_list = kwargs['held_out_list']\n",
    "\n",
    "        assert held_out_list is None, 'inter_region does LOO for all neurons in the target region'\n",
    "\n",
    "        gt_result_list, pred_result_list = [], []\n",
    "        y_list, y_pred_list, y_residual_list = [], [], []\n",
    "        bps_result_list, r2_result_list = [float('nan')] * tot_num_neurons, [np.array([np.nan, np.nan])] * N\n",
    "        for region in tqdm(target_regions, desc='region'):\n",
    "            print(region)\n",
    "            hd = np.argwhere(region_list==region).flatten() \n",
    "            held_out_list = np.arange(len(hd))\n",
    "            held_out_list = [held_out_list]   \n",
    "            hd = np.array([held_out_list]).flatten()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in test_dataloader:\n",
    "                    batch = move_batch_to_device(batch, accelerator.device)       \n",
    "                        \n",
    "                    gt_spike_data = batch['spikes_data'].clone()\n",
    "                    mask_result = heldout_mask(\n",
    "                        batch['spikes_data'].clone(),\n",
    "                        mode=mode,\n",
    "                        heldout_idxs=hd,\n",
    "                        target_regions=[region],\n",
    "                        neuron_regions=region_list\n",
    "                    )              \n",
    "\n",
    "                    try:\n",
    "                        masking_mode = 'inter-region' if model.use_prompt else model.encoder.masker.mode\n",
    "                        model.encoder.mask = False\n",
    "                    except AttributeError:\n",
    "                        masking_mode = 'inter-region' if model.use_prompt else model.masker.mode\n",
    "                        model.mask = False\n",
    "                    \n",
    "                    outputs = model(\n",
    "                        mask_result['spikes'],\n",
    "                        time_attn_mask=batch['time_attn_mask'],\n",
    "                        space_attn_mask=batch['space_attn_mask'],\n",
    "                        spikes_timestamps=batch['spikes_timestamps'], \n",
    "                        spikes_spacestamps=batch['spikes_spacestamps'], \n",
    "                        targets = batch['target'],\n",
    "                        neuron_regions=batch['neuron_regions'],\n",
    "                        eval_mask=mask_result['eval_mask'],\n",
    "                        masking_mode=masking_mode,\n",
    "                        num_neuron=batch['spikes_data'].shape[2],\n",
    "                        eid=batch['eid'][0],\n",
    "                    )\n",
    "            outputs.preds = torch.exp(outputs.preds)\n",
    "        \n",
    "            gt_spikes = gt_spike_data.detach().cpu().numpy()\n",
    "            pred_spikes = outputs.preds.detach().cpu().numpy()\n",
    "    \n",
    "            target_neuron_idxs = mask_result['heldout_idxs']\n",
    "            target_time_idxs = np.arange(gt_spikes.shape[1])\n",
    "\n",
    "            # compute co-bps\n",
    "            gt_held_out = gt_spikes[:, target_time_idxs][:,:,target_neuron_idxs]\n",
    "            pred_held_out = pred_spikes[:, target_time_idxs][:,:,target_neuron_idxs]\n",
    "\n",
    "            bps_per_region = []\n",
    "            for n_i in range(len(target_neuron_idxs)): \n",
    "                bps = bits_per_spike(pred_held_out[:,:,[n_i]], gt_held_out[:,:,[n_i]])\n",
    "                if np.isinf(bps):\n",
    "                    bps = np.nan\n",
    "                bps_result_list[target_neuron_idxs[n_i]] = bps\n",
    "                bps_per_region.append(bps)\n",
    "            bps_per_region = [bits_per_spike(pred_held_out, gt_held_out)]\n",
    "            print(f'{region} bps: ', np.nanmean(bps_per_region))\n",
    "\n",
    "            # compute R2\n",
    "            ys = gt_spikes[:, target_time_idxs]\n",
    "            y_preds = pred_spikes[:, target_time_idxs]\n",
    "    \n",
    "            # choose the neuron to plot\n",
    "            idxs = target_neuron_idxs\n",
    "            for i in range(idxs.shape[0]):\n",
    "\n",
    "                gt_result_list.append(ys[:, :, idxs[i]])\n",
    "                pred_result_list.append(y_preds[:, :, idxs[i]])\n",
    "            \n",
    "                if is_aligned:\n",
    "                    X = behavior_set[:, target_time_idxs, :]  # [#trials, #timesteps, #variables]\n",
    "                    _r2_psth, _r2_trial, y, y_pred, y_residual = viz_single_cell(X, ys[:, :, idxs[i]], y_preds[:, :, idxs[i]],\n",
    "                                                          var_name2idx, var_tasklist, var_value2label, var_behlist,\n",
    "                                                          subtract_psth=kwargs['subtract'],\n",
    "                                                          aligned_tbins=[],\n",
    "                                                          neuron_idx=uuids_list[idxs[i]][:4],\n",
    "                                                          neuron_region=region_list[idxs[i]],\n",
    "                                                          method=method_name, save_path=kwargs['save_path'],\n",
    "                                                          save_plot=save_plot\n",
    "                                                         );\n",
    "                    r2_result_list[idxs[i]] = np.array([_r2_psth, _r2_trial])\n",
    "\n",
    "                    y_list.append(y)\n",
    "                    y_pred_list.append(y_pred)\n",
    "                    y_residual_list.append(y_residual)\n",
    "                    \n",
    "                else:\n",
    "                    r2 = viz_single_cell_unaligned(\n",
    "                        ys[:, :, idxs[i]], y_preds[:, :, idxs[i]], \n",
    "                        neuron_idx=uuids_list[idxs[i]][:4],\n",
    "                        neuron_region=region_list[idxs[i]],\n",
    "                        method=method_name, save_path=kwargs['save_path'],\n",
    "                        save_plot=save_plot\n",
    "                    )\n",
    "                    r2_result_list[idxs[i]] = r2\n",
    "\n",
    "            break\n",
    "                        \n",
    "    elif mode == 'intra_region':\n",
    "\n",
    "        if 'all' in target_regions:\n",
    "            target_regions = list(np.unique(region_list))\n",
    "            \n",
    "        held_out_list = kwargs['held_out_list']\n",
    "        assert held_out_list is None, 'intra_region does LOO for all neurons in the target region'\n",
    "\n",
    "        gt_result_list, pred_result_list = [], []\n",
    "        y_list, y_pred_list, y_residual_list = [], [], []\n",
    "        bps_result_list, r2_result_list = [float('nan')] * tot_num_neurons, [np.array([np.nan, np.nan])] * N\n",
    "        for region in tqdm(target_regions, desc='region'):\n",
    "            print(region)\n",
    "            target_neuron_idxs = np.argwhere(region_list==region).flatten() \n",
    "            held_out_list = list(range(0, len(target_neuron_idxs)+n_jobs, n_jobs))\n",
    "\n",
    "            bps_per_region = []\n",
    "            \n",
    "            for hd_idx in held_out_list:\n",
    "                \n",
    "                if hd_idx >= len(target_neuron_idxs):\n",
    "                    break\n",
    "\n",
    "                gt_spikes_lst, mask_spikes_lst, eval_mask_lst, heldout_idxs_lst = [], [], [], []\n",
    "                time_attn_mask_lst, space_attn_mask_lst, spikes_timestamps_lst, spikes_spacestamps_lst, targets_lst, neuron_regions_lst, nemo_rep_lst = [], [], [], [], [], [], []\n",
    "    \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_dataloader:\n",
    "                        batch = move_batch_to_device(batch, accelerator.device)\n",
    "\n",
    "                        gt_spike_data = batch['spikes_data'].clone()\n",
    "                        for i in range(n_jobs):\n",
    "                            if hd_idx+i < len(target_neuron_idxs):\n",
    "                                mask_result = heldout_mask(\n",
    "                                    batch['spikes_data'].clone(),\n",
    "                                    mode=mode,\n",
    "                                    heldout_idxs=np.array([hd_idx+i]).flatten(),\n",
    "                                    target_regions=[region],\n",
    "                                    neuron_regions=region_list\n",
    "                                )   \n",
    "                                mask_spikes_lst.append(mask_result['spikes'])\n",
    "                                eval_mask_lst.append(mask_result['eval_mask'])\n",
    "                                heldout_idxs_lst.append(mask_result['heldout_idxs'])\n",
    "                                gt_spikes_lst.append(gt_spike_data)\n",
    "                                time_attn_mask_lst.append(batch['time_attn_mask'])\n",
    "                                space_attn_mask_lst.append(batch['space_attn_mask'])\n",
    "                                spikes_timestamps_lst.append(batch['spikes_timestamps'])\n",
    "                                spikes_spacestamps_lst.append(batch['spikes_spacestamps'])\n",
    "                                targets_lst.append(batch['target'])\n",
    "                                neuron_regions_lst.append(batch['neuron_regions'])\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "                        try:\n",
    "                            masking_mode = 'intra-region' if model.use_prompt else model.encoder.masker.mode\n",
    "                            model.encoder.mask = False\n",
    "                        except AttributeError:\n",
    "                            masking_mode = 'intra-region' if model.use_prompt else model.masker.mode\n",
    "                            model.mask = False\n",
    "                        \n",
    "                        outputs = model(\n",
    "                            torch.cat(mask_spikes_lst, 0),\n",
    "                            time_attn_mask=torch.cat(time_attn_mask_lst, 0),\n",
    "                            space_attn_mask=torch.cat(space_attn_mask_lst, 0),\n",
    "                            spikes_timestamps=torch.cat(spikes_timestamps_lst, 0), \n",
    "                            spikes_spacestamps=torch.cat(spikes_spacestamps_lst, 0), \n",
    "                            targets = torch.cat(targets_lst, 0),\n",
    "                            neuron_regions=np.stack(neuron_regions_lst, axis=-1).squeeze(),\n",
    "                            eval_mask=torch.cat(eval_mask_lst, 0),\n",
    "                            masking_mode=masking_mode,\n",
    "                            num_neuron=batch['spikes_data'].shape[2],\n",
    "                            eid=batch['eid'][0],\n",
    "                        )\n",
    "                outputs.preds = torch.exp(outputs.preds)\n",
    "            \n",
    "                gt_spikes = torch.cat(gt_spikes_lst, 0).detach().cpu().numpy()\n",
    "                pred_spikes = outputs.preds.detach().cpu().numpy()\n",
    "                tot_num_trials = len(batch['spikes_data'])\n",
    "\n",
    "                heldout_idxs = np.stack(heldout_idxs_lst).flatten()\n",
    "                \n",
    "                for i in range(len(heldout_idxs)):\n",
    "                    gt_held_out = gt_spikes[i*tot_num_trials:(i+1)*tot_num_trials, :, [heldout_idxs[i]]]\n",
    "                    pred_held_out = pred_spikes[i*tot_num_trials:(i+1)*tot_num_trials, :, [heldout_idxs[i]]]\n",
    "    \n",
    "                    # bps = bits_per_spike(pred_held_out, gt_held_out)\n",
    "                    # if np.isinf(bps):\n",
    "                    #     bps = np.nan\n",
    "                    # bps_result_list[heldout_idxs[i]] = bps\n",
    "                    # bps_per_region.append(bps)\n",
    "\n",
    "                    gt_result_list.append(gt_held_out.squeeze())\n",
    "                    pred_result_list.append(pred_held_out.squeeze())\n",
    "\n",
    "                    if is_aligned:\n",
    "                        X = behavior_set  # [#trials, #timesteps, #variables]\n",
    "                        _r2_psth, _r2_trial, y, y_pred, y_residual = viz_single_cell(X, gt_held_out.squeeze(), pred_held_out.squeeze(),\n",
    "                                                              var_name2idx, var_tasklist, var_value2label, var_behlist,\n",
    "                                                              subtract_psth=kwargs['subtract'],\n",
    "                                                              aligned_tbins=[],\n",
    "                                                              neuron_idx=uuids_list[heldout_idxs[i]][:4],\n",
    "                                                              neuron_region=region_list[heldout_idxs[i]],\n",
    "                                                              method=method_name, save_path=kwargs['save_path'],\n",
    "                                                              save_plot=save_plot\n",
    "                                                             );\n",
    "                        r2_result_list[heldout_idxs[i]] = np.array([_r2_psth, _r2_trial])\n",
    "                        y_list.append(y)\n",
    "                        y_pred_list.append(y_pred)\n",
    "                        y_residual_list.append(y_residual)\n",
    "                    else:\n",
    "                        r2 = viz_single_cell_unaligned(\n",
    "                            gt_held_out.squeeze(), pred_held_out.squeeze(),\n",
    "                            neuron_idx=uuids_list[heldout_idxs[i]][:4],\n",
    "                            neuron_region=region_list[heldout_idxs[i]],\n",
    "                            method=method_name, save_path=kwargs['save_path'],\n",
    "                            save_plot=save_plot\n",
    "                        )\n",
    "                        r2_result_list[heldout_idxs[i]] = r2\n",
    "\n",
    "            bps_per_region = [bits_per_spike(\n",
    "                np.array(pred_result_list).transpose(1,2,0), \n",
    "                np.array(gt_result_list).transpose(1,2,0)\n",
    "            )]\n",
    "            print(f'{region} bps: ', np.nanmean(bps_per_region))\n",
    "    else:\n",
    "        raise NotImplementedError('mode not implemented')\n",
    "\n",
    "    # save co-bps\n",
    "    os.makedirs(kwargs['save_path'], exist_ok=True)\n",
    "    bps_all = np.array(bps_result_list)\n",
    "    bps_mean = np.nanmean(bps_all)\n",
    "    bps_std = np.nanstd(bps_all)\n",
    "    np.save(os.path.join(kwargs['save_path'], f'bps.npy'), bps_all)\n",
    "    \n",
    "    # save R2\n",
    "    r2_all = np.array(r2_result_list)\n",
    "    np.save(os.path.join(kwargs['save_path'], f'r2.npy'), r2_all)\n",
    "\n",
    "    return gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "589cd784-4da8-4c0a-9893-60fc0ffbf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# single neuron plot functions\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    ":X: [n_trials, n_timesteps, n_variables]\n",
    ":y: [n_trials, n_timesteps] (in Hz)\n",
    ":y_pred: [n_trials, n_timesteps] (in Hz)\n",
    ":var_tasklist: for each task variable in var_tasklists, compute PSTH\n",
    ":var_name2idx: for each task variable in var_tasklists, the corresponding index of X\n",
    ":var_value2label:\n",
    ":aligned_tbins: reference time steps to annotate. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_psth(X, y, y_pred, var_tasklist, var_name2idx, var_value2label,\n",
    "              aligned_tbins=[],\n",
    "              axes=None, legend=False, neuron_idx='', neuron_region='', save_plot=False):\n",
    "    \n",
    "    if save_plot:\n",
    "        if axes is None:\n",
    "            nrows = 1;\n",
    "            ncols = len(var_tasklist)\n",
    "            fig, axes = plt.subplots(nrows, ncols, figsize=(3 * ncols, 2 * nrows))\n",
    "\n",
    "        for ci, var in enumerate(var_tasklist):\n",
    "            ax = axes[ci]\n",
    "            psth_xy = compute_all_psth(X, y, var_name2idx[var])\n",
    "            psth_pred_xy = compute_all_psth(X, y_pred, var_name2idx[var])\n",
    "            \n",
    "            for _i, _x in enumerate(psth_xy.keys()):\n",
    "                psth = psth_xy[_x]\n",
    "                psth_pred = psth_pred_xy[_x]\n",
    "                ax.plot(psth,\n",
    "                        color=plt.get_cmap('tab10')(_i),\n",
    "                        linewidth=3, alpha=0.3, label=f\"{var}: {tuple(_x)[0]:.2f}\")\n",
    "                ax.plot(psth_pred,\n",
    "                        color=plt.get_cmap('tab10')(_i),\n",
    "                        linestyle='--')\n",
    "                ax.set_xlabel(\"Time bin\")\n",
    "                if ci == 0:\n",
    "                    ax.set_ylabel(\"Neural activity\")\n",
    "                else:\n",
    "                    ax.sharey(axes[0])\n",
    "            _add_baseline(ax, aligned_tbins=aligned_tbins)\n",
    "            if legend:\n",
    "                ax.legend()\n",
    "                ax.set_title(f\"{var}\")\n",
    "\n",
    "    # compute PSTH for task_contingency\n",
    "    idxs_psth = np.concatenate([var_name2idx[var] for var in var_tasklist])\n",
    "    psth_xy = compute_all_psth(X, y, idxs_psth)\n",
    "    psth_pred_xy = compute_all_psth(X, y_pred, idxs_psth)\n",
    "    r2_psth = compute_R2_psth(psth_xy, psth_pred_xy, clip=False)\n",
    "    r2_single_trial = compute_R2_main(y.reshape(-1, 1), y_pred.reshape(-1, 1), clip=False)[0]\n",
    "    \n",
    "    if save_plot:\n",
    "        axes[0].set_ylabel(\n",
    "            f'Neuron: #{neuron_idx[:4]} \\n PSTH R2: {r2_psth:.2f} \\n Avg_SingleTrial R2: {r2_single_trial:.2f}')\n",
    "\n",
    "        for ax in axes:\n",
    "            # ax.axis('off')\n",
    "            ax.spines[['right', 'top']].set_visible(False)\n",
    "            # ax.set_frame_on(False)\n",
    "            # ax.tick_params(bottom=False, left=False)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    return r2_psth, r2_single_trial\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ":X: [n_trials, n_timesteps, n_variables]\n",
    ":y: [n_trials, n_timesteps] (in Hz)\n",
    ":y_pred: [n_trials, n_timesteps] (in Hz)\n",
    ":var_tasklist: variables used for computing the task-condition-averaged psth if subtract_psth=='task'\n",
    ":var_name2idx:\n",
    ":var_tasklist: variables to be plotted in the single-trial behavior\n",
    ":subtract_psth: \n",
    "    - None: no subtraction\n",
    "    - \"task\": subtract task-condition-averaged psth\n",
    "    - \"global\": subtract global-averaged psth\n",
    ":aligned_tbins: reference time steps to annotate. \n",
    ":nclus, n_neighbors: hyperparameters for spectral_clustering\n",
    ":cmap, vmax_perc, vmin_perc: parameters used when plotting the activity and behavior\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def plot_single_trial_activity(X, y, y_pred,\n",
    "                               var_name2idx,\n",
    "                               var_behlist,\n",
    "                               var_tasklist, subtract_psth=\"task\",\n",
    "                               aligned_tbins=[],\n",
    "                               n_clus=8, n_neighbors=5, n_pc=32, clusby='y_pred',\n",
    "                               cmap='bwr', vmax_perc=90, vmin_perc=10,\n",
    "                               axes=None):\n",
    "    if axes is None:\n",
    "        ncols = 1;\n",
    "        nrows = 2 + len(var_behlist) + 1 + 1\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(8 * ncols, 3 * nrows))\n",
    "\n",
    "    ### get the psth-subtracted y\n",
    "    if subtract_psth is None:\n",
    "        pass\n",
    "    elif subtract_psth == \"task\":\n",
    "        idxs_psth = np.concatenate([var_name2idx[var] for var in var_tasklist])\n",
    "        psth_xy = compute_all_psth(X, y, idxs_psth)\n",
    "        psth_pred_xy = compute_all_psth(X, y_pred, idxs_psth)\n",
    "        y_psth = np.asarray(\n",
    "            [psth_xy[tuple(x)] for x in X[:, 0, idxs_psth]])  # (K, T) predict the neural activity with psth\n",
    "        y_predpsth = np.asarray(\n",
    "            [psth_pred_xy[tuple(x)] for x in X[:, 0, idxs_psth]])  # (K, T) predict the neural activity with psth\n",
    "        # y = y - y_psth  # (K, T)\n",
    "        # y_pred = y_pred - y_predpsth  # (K, T)\n",
    "        y = y - y_psth  # (K, T)\n",
    "        y_pred = y_pred - y_psth  # (K, T)\n",
    "    elif subtract_psth == \"global\":\n",
    "        y_psth = np.mean(y, 0)\n",
    "        y_predpsth = np.mean(y_pred, 0)\n",
    "        y = y - y_psth  # (K, T)\n",
    "        y_pred = y_pred - y_predpsth  # (K, T)\n",
    "    else:\n",
    "        assert False, \"Unknown subtract_psth, has to be one of: task, global. \\'\\'\"\n",
    "    y_residual = (y_pred - y)  # (K, T), residuals of prediction\n",
    "    idxs_behavior = np.concatenate(([var_name2idx[var] for var in var_behlist])) if len(var_behlist) > 0 else []\n",
    "    X_behs = X[:, :, idxs_behavior]\n",
    "\n",
    "    clustering = SpectralClustering(n_clusters=n_clus, n_neighbors=n_neighbors,\n",
    "                                    affinity='nearest_neighbors',\n",
    "                                    assign_labels='discretize',\n",
    "                                    random_state=0)\n",
    "    if clusby == 'y_pred':\n",
    "        clustering = clustering.fit(y_pred)\n",
    "    elif clusby == 'y':\n",
    "        clustering = clustering.fit(y)\n",
    "    else:\n",
    "        assert False, \"invalid clusby\"\n",
    "    t_sort = np.argsort(clustering.labels_)\n",
    "\n",
    "    for ri, (toshow, label, ax) in enumerate(zip([y, y_pred, X_behs, y_residual],\n",
    "                                                 [f\"obs. act. \\n (subtract_psth={subtract_psth})\",\n",
    "                                                  f\"pred. act. \\n (subtract_psth={subtract_psth})\",\n",
    "                                                  var_behlist,\n",
    "                                                  \"residual act.\"],\n",
    "                                                 [axes[0], axes[1], axes[2:-2], axes[-2]])):\n",
    "        if ri <= 1:\n",
    "            # plot obs./ predicted activity\n",
    "            vmax = np.percentile(y_pred, vmax_perc)\n",
    "            vmin = np.percentile(y_pred, vmin_perc)\n",
    "            raster_plot(toshow[t_sort], vmax, vmin, True, label, ax,\n",
    "                        cmap=cmap,\n",
    "                        aligned_tbins=aligned_tbins)\n",
    "        elif ri == 2:\n",
    "            # plot behavior\n",
    "            for bi in range(len(var_behlist)):\n",
    "                ts_ = toshow[:, :, bi][t_sort]\n",
    "                vmax = np.percentile(ts_, vmax_perc)\n",
    "                vmin = np.percentile(ts_, vmin_perc)\n",
    "                raster_plot(ts_, vmax, vmin, True, label[bi], ax[bi],\n",
    "                            cmap=cmap,\n",
    "                            aligned_tbins=aligned_tbins)\n",
    "        elif ri == 3:\n",
    "            # plot residual activity\n",
    "            vmax = np.percentile(toshow, vmax_perc)\n",
    "            vmin = np.percentile(toshow, vmin_perc)\n",
    "            raster_plot(toshow[t_sort], vmax, vmin, True, label, ax,\n",
    "                        cmap=cmap,\n",
    "                        aligned_tbins=aligned_tbins)\n",
    "\n",
    "    ### plot single-trial activity\n",
    "    # re-arrange the trials\n",
    "    clustering = SpectralClustering(n_clusters=n_clus, n_neighbors=n_neighbors,\n",
    "                                    affinity='nearest_neighbors',\n",
    "                                    assign_labels='discretize',\n",
    "                                    random_state=0).fit(y_residual)\n",
    "    t_sort_rd = np.argsort(clustering.labels_)\n",
    "    # model = Rastermap(n_clusters=n_clus, n_PCs=n_pc, locality=0.15, time_lag_window=15, grid_upsample=0,).fit(y_residual)\n",
    "    # t_sort_rd = model.isort\n",
    "    raster_plot(y_residual[t_sort_rd], np.percentile(y_residual, vmax_perc), np.percentile(y_residual, vmin_perc), True,\n",
    "                \"residual act. (re-clustered)\", axes[-1])\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    return y, y_pred, y_residual\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script generates a plot to examine the (single-trial) fitting of a single neuron.\n",
    ":X: behavior matrix of the shape [n_trials, n_timesteps, n_variables]. \n",
    ":y: true neural activity matrix of the shape [n_trials, n_timesteps] \n",
    ":ypred: predicted activity matrix of the shape [n_trials, n_timesteps] \n",
    ":var_name2idx: dictionary mapping feature names to their corresponding index of the 3-rd axis of the behavior matrix X. e.g.: {\"choice\": [0], \"wheel\": [1]}\n",
    ":var_tasklist: *static* task variables used to form the task condition and compute the psth. e.g.: [\"choice\"]\n",
    ":var_value2label: dictionary mapping values in X to their corresponding readable labels (only required for static task variables). e.g.: {\"choice\": {1.: \"left\", -1.: \"right\"}}\n",
    ":var_behlist: *dynamic* behavior variables. e.g., [\"wheel\"]\n",
    ":subtract_psth: \n",
    "    - None: no subtraction\n",
    "    - \"task\": subtract task-condition-averaged psth\n",
    "    - \"global\": subtract global-averaged psth\n",
    ":algined_tbins: reference time steps to annotate in the plot. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def viz_single_cell(X, y, y_pred, var_name2idx, var_tasklist, var_value2label, var_behlist,\n",
    "                    subtract_psth=\"task\", aligned_tbins=[], clusby='y_pred', neuron_idx='', neuron_region='', method='',\n",
    "                    save_path='figs', save_plot=False):\n",
    "    \n",
    "    if save_plot:\n",
    "        nrows = 8\n",
    "        plt.figure(figsize=(8, 2 * nrows))\n",
    "        axes_psth = [plt.subplot(nrows, len(var_tasklist), k + 1) for k in range(len(var_tasklist))]\n",
    "        axes_single = [plt.subplot(nrows, 1, k) for k in range(2, 2 + 2 + len(var_behlist) + 2)]\n",
    "    else:\n",
    "        axes_psth = None\n",
    "        axes_single = None\n",
    "\n",
    "\n",
    "    ### plot psth\n",
    "    r2_psth, r2_trial = plot_psth(X, y, y_pred,\n",
    "                                  var_tasklist=var_tasklist,\n",
    "                                  var_name2idx=var_name2idx,\n",
    "                                  var_value2label=var_value2label,\n",
    "                                  aligned_tbins=aligned_tbins,\n",
    "                                  axes=axes_psth, legend=True, neuron_idx=neuron_idx, neuron_region=neuron_region,\n",
    "                                  save_plot=save_plot)\n",
    "\n",
    "    ### plot the psth-subtracted activity\n",
    "    if save_plot:\n",
    "        y, y_pred, y_residual = plot_single_trial_activity(X, y, y_pred,\n",
    "                                   var_name2idx,\n",
    "                                   var_behlist,\n",
    "                                   var_tasklist, subtract_psth=subtract_psth,\n",
    "                                   aligned_tbins=aligned_tbins,\n",
    "                                   clusby=clusby,\n",
    "                                   axes=axes_single)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    if save_plot:\n",
    "        plt.savefig(os.path.join(save_path, f\"{neuron_region.replace('/', '-')}_{neuron_idx}_{r2_trial:.2f}_{method}.png\"))\n",
    "        plt.tight_layout();\n",
    "\n",
    "    return r2_psth, r2_trial, y, y_pred, y_residual\n",
    "    \n",
    "\n",
    "def viz_single_cell_unaligned(\n",
    "    gt, pred, neuron_idx, neuron_region, method, save_path, \n",
    "    n_clus=8, n_neighbors=5, save_plot=False\n",
    "):\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    r2 = 0\n",
    "    for _ in range(len(gt)):\n",
    "        r2 += r2_score(gt, pred)\n",
    "    r2 /= len(gt)\n",
    "\n",
    "    if save_plot:\n",
    "        y = gt - gt.mean(0)\n",
    "        y_pred = pred - pred.mean(0)\n",
    "        y_resid = y - y_pred\n",
    "\n",
    "        clustering = SpectralClustering(n_clusters=n_clus, n_neighbors=n_neighbors,\n",
    "                                            affinity='nearest_neighbors',\n",
    "                                            assign_labels='discretize',\n",
    "                                            random_state=0)\n",
    "\n",
    "        clustering = clustering.fit(y_pred)\n",
    "        t_sort = np.argsort(clustering.labels_)\n",
    "        \n",
    "        vmin_perc, vmax_perc = 10, 90 \n",
    "        vmax = np.percentile(y_pred, vmax_perc)\n",
    "        vmin = np.percentile(y_pred, vmin_perc)\n",
    "        \n",
    "        toshow = [y, y_pred, y_resid]\n",
    "        resid_vmax = np.percentile(toshow, vmax_perc)\n",
    "        resid_vmin = np.percentile(toshow, vmin_perc)\n",
    "        \n",
    "        N = len(y)\n",
    "        y_labels = ['obs.', 'pred.', 'resid.']\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 7))\n",
    "        norm = colors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "        im1 = axes[0].imshow(y[t_sort], aspect='auto', cmap='bwr', norm=norm)\n",
    "        cbar = plt.colorbar(im1, pad=0.02, shrink=.6)\n",
    "        cbar.ax.tick_params(rotation=90)\n",
    "        axes[0].set_title(f' R2: {r2:.3f}')\n",
    "        norm = colors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
    "        im2 = axes[1].imshow(y_pred[t_sort], aspect='auto', cmap='bwr', norm=norm)\n",
    "        cbar = plt.colorbar(im2, pad=0.02, shrink=.6)\n",
    "        cbar.ax.tick_params(rotation=90)\n",
    "        norm = colors.TwoSlopeNorm(vmin=resid_vmin, vcenter=0, vmax=resid_vmax)\n",
    "        im3 = axes[2].imshow(y_resid[t_sort], aspect='auto', cmap='bwr', norm=norm)\n",
    "        cbar = plt.colorbar(im3, pad=0.02, shrink=.6)\n",
    "        cbar.ax.tick_params(rotation=90)\n",
    "        \n",
    "        for i, ax in enumerate(axes):\n",
    "            ax.set_ylabel(f\"{y_labels[i]}\"+f\"\\n(#trials={N})\")\n",
    "            ax.yaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticklabels([])\n",
    "            ax.xaxis.set_ticks([])\n",
    "            ax.xaxis.set_ticklabels([])\n",
    "            ax.spines[['left','bottom', 'right', 'top']].set_visible(False)\n",
    "        \n",
    "        plt.savefig(os.path.join(save_path, f\"{neuron_region.replace('/', '-')}_{neuron_idx}_{r2:.2f}_{method}.png\"))\n",
    "        plt.tight_layout()\n",
    "\n",
    "    return r2\n",
    "\n",
    "\n",
    "def _add_baseline(ax, aligned_tbins=[40]):\n",
    "    for tbin in aligned_tbins:\n",
    "        ax.axvline(x=tbin - 1, c='k', alpha=0.2)\n",
    "    # ax.axhline(y=0., c='k', alpha=0.2)\n",
    "\n",
    "\n",
    "def raster_plot(ts_, vmax, vmin, whether_cbar, ylabel, ax,\n",
    "                cmap='bwr',\n",
    "                aligned_tbins=[40]):\n",
    "    N, T = ts_.shape\n",
    "    im = ax.imshow(ts_, aspect='auto', cmap=cmap, vmax=vmax, vmin=vmin)\n",
    "    for tbin in aligned_tbins:\n",
    "        ax.annotate('',\n",
    "                    xy=(tbin - 1, N),\n",
    "                    xytext=(tbin - 1, N + 10),\n",
    "                    ha='center',\n",
    "                    va='center',\n",
    "                    arrowprops={'arrowstyle': '->', 'color': 'r'})\n",
    "    if whether_cbar:\n",
    "        cbar = plt.colorbar(im, pad=0.01, shrink=.6)\n",
    "        cbar.ax.tick_params(rotation=90)\n",
    "    if not (ylabel is None):\n",
    "        ax.set_ylabel(f\"{ylabel}\" + f\"\\n(#trials={N})\")\n",
    "        ax.yaxis.set_ticks([])\n",
    "        ax.yaxis.set_ticklabels([])\n",
    "        ax.xaxis.set_ticks([])\n",
    "        ax.xaxis.set_ticklabels([])\n",
    "        ax.spines[['left', 'bottom', 'right', 'top']].set_visible(False)\n",
    "        pass\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "- X, y should be nparray with\n",
    "    - X: [K,T,ncoef]\n",
    "    - y: [K,T,N] or [K,T]\n",
    "- axis and value should be list\n",
    "- return: nparray [T, N] or [T]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compute_PSTH(X, y, axis, value):\n",
    "    trials = np.all(X[:, 0, axis] == value, axis=-1)\n",
    "    return y[trials].mean(0)\n",
    "\n",
    "\n",
    "def compute_all_psth(X, y, idxs_psth):\n",
    "    uni_vs = np.unique(X[:, 0, idxs_psth], axis=0)  # get all the unique task-conditions\n",
    "    psth_vs = {};\n",
    "    for v in uni_vs:\n",
    "        # compute separately for true y and predicted y\n",
    "        _psth = compute_PSTH(X, y,\n",
    "                             axis=idxs_psth, value=v)  # (T)\n",
    "        psth_vs[tuple(v)] = _psth\n",
    "    return psth_vs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "psth_xy/ psth_pred_xy: {tuple(x): (T) or (T,N)}\n",
    "return a float or (N) array\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def compute_R2_psth(psth_xy, psth_pred_xy, clip=True):\n",
    "    psth_xy_array = np.array([psth_xy[x] for x in psth_xy])\n",
    "    psth_pred_xy_array = np.array([psth_pred_xy[x] for x in psth_xy])\n",
    "    K, T = psth_xy_array.shape[:2]\n",
    "    psth_xy_array = psth_xy_array.reshape((K * T, -1))\n",
    "    psth_pred_xy_array = psth_pred_xy_array.reshape((K * T, -1))\n",
    "    r2s = [r2_score(psth_xy_array[:, ni], psth_pred_xy_array[:, ni]) for ni in range(psth_xy_array.shape[1])]\n",
    "    r2s = np.array(r2s)\n",
    "    # # compute r2 along dim 0\n",
    "    # r2s = [r2_score(psth_xy[x], psth_pred_xy[x], multioutput='raw_values') for x in psth_xy]\n",
    "    if clip:\n",
    "        r2s = np.clip(r2s, 0., 1.)\n",
    "    # r2s = np.mean(r2s, 0)\n",
    "    if len(r2s) == 1:\n",
    "        r2s = r2s[0]\n",
    "    return r2s\n",
    "\n",
    "\n",
    "def compute_R2_main(y, y_pred, clip=True):\n",
    "    \"\"\"\n",
    "    :y: (K, T, N) or (K*T, N)\n",
    "    :y_pred: (K, T, N) or (K*T, N)\n",
    "    \"\"\"\n",
    "    N = y.shape[-1]\n",
    "    if len(y.shape) > 2:\n",
    "        y = y.reshape((-1, N))\n",
    "    if len(y_pred.shape) > 2:\n",
    "        y_pred = y_pred.reshape((-1, N))\n",
    "    r2s = np.asarray([r2_score(y[:, n].flatten(), y_pred[:, n].flatten()) for n in range(N)])\n",
    "    if clip:\n",
    "        return np.clip(r2s, 0., 1.)\n",
    "    else:\n",
    "        return r2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e817ab20-19a5-4204-b9f6-42f28384f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/u/csanthirasegaran'\n",
    "eid = '51e53aff-1d5d-4182-a684-aba783d50ae5'\n",
    "model_acronym = 'ndt1'\n",
    "model_name = 'NDT1'\n",
    "model_config = f\"src/configs/{model_acronym}_eval.yaml\"\n",
    "mask_ratio = 0.3\n",
    "best_ckpt_path = 'model_best.pt'\n",
    "tokenize_binary_mask = False    #was True in the other notebook\n",
    "threshold = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141b4f5-0de5-41ae-bca8-c97690730e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_trial_idxs = np.load(f'notebooks/trial_idxs/{eid}_randomized.npy', allow_pickle=True).item()\n",
    "# nonrandom_trial_idxs = np.load(f'notebooks/trial_idxs/{eid}_nonrandomized.npy', allow_pickle=True).item()\n",
    "# overlap_trial_idxs = np.intersect1d(random_trial_idxs['test'], nonrandom_trial_idxs['test'])\n",
    "\n",
    "# keep_trial_idxs = []\n",
    "# for idx in overlap_trial_idxs:\n",
    "#     keep_trial_idxs.append(np.argwhere(random_trial_idxs['test'] == idx))\n",
    "# keep_trial_idxs = np.array(keep_trial_idxs).flatten()\n",
    "\n",
    "# print('Ratio: ', len(keep_trial_idxs)/len(random_trial_idxs['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fff2e-cc5d-46c7-989c-c806b6751550",
   "metadata": {},
   "source": [
    "#### Inter-Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2030d143-b7ea-443a-95d8-59698d832320",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'inter_region'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c026aa-8d60-436e-a17f-4112501e1c36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/csanthirasegaran/IBL_foundation_model/src/utils/dataset_utils.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  all_datasets = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total session-wise datasets found:  39\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'51e53aff-1d5d-4182-a684-aba783d50ae5'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  336\n",
      "Val dataset size:  48\n",
      "Test dataset size:  96\n",
      "encoder max space length: 862\n",
      "{'num_neurons': [862], 'num_sessions': 1, 'eids': {'51e53aff-1d5d-4182-a684-aba783d50ae5'}, 'tokenize_binary_mask': False, 'max_space_length': 862}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "len(dataset): 101\n",
      "spike data shape: torch.Size([101, 100, 862])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_temporal'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37f3958e-706a-4aa7-9fc1-9e638336cf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [00:03<00:28,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [00:06<00:27,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [00:10<00:25,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [00:14<00:22,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [00:18<00:19,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [00:21<00:14,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [00:24<00:10,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [00:27<00:06,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [00:30<00:03,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [00:33<00:00,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.3288218558877312\n",
      "Population bps:  0.21839956445212466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22daa0a-0c92-45f6-8402-e21a84b351fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fb6cfdd-e4e7-447e-8f97-6be051404d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  temporal\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_temporal'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9974d7f-9823-49e9-ae0c-e2c1c547c94a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [00:04<00:39,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [00:08<00:31,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [00:11<00:26,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [00:14<00:21,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [00:17<00:17,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [00:21<00:13,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [00:24<00:09,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [00:27<00:06,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [00:31<00:03,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [00:35<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: -0.15338643409738584\n",
      "Population bps:  -0.1991331649935558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d721bb-bf3d-46f8-adde-8f04b2e38326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37ac70c6-c329-471b-9cad-edc0583c5d09",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e4b47f6-17f4-4ec9-a347-339fff145fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [00:04<00:37,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [00:07<00:28,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [00:10<00:25,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [00:13<00:20,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [00:16<00:16,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [00:19<00:12,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [00:22<00:09,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [00:25<00:06,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [00:29<00:03,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [00:32<00:00,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.5118597724748539\n",
      "Population bps:  0.37828861028457417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866f1b2-b443-43aa-9cf2-e333079bdff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb7e9934-8155-43b1-995a-f246443c3814",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21cd4077-a40a-4360-adec-0b76a9d7a70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [00:04<00:37,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [00:07<00:28,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [00:10<00:23,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [00:13<00:19,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [00:16<00:16,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [00:19<00:12,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [00:23<00:09,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [00:26<00:06,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [00:29<00:03,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [00:32<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.10519980803857949\n",
      "Population bps:  0.01501113349935487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670cc885-038a-49fd-b548-d2b468d4bf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5787549-201d-4ddc-b9f7-6ef1a6a9fe88",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.6\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91ba786d-357b-4475-9032-28814d253703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [00:04<00:37,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [00:07<00:29,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [00:10<00:23,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [00:14<00:21,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [00:17<00:17,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [00:20<00:13,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [00:24<00:09,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [00:27<00:06,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [00:30<00:03,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [00:33<00:00,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.5669529444007689\n",
      "Population bps:  0.4061118345541712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a96ff-1ba7-484f-8901-24b191cabd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e03d2f2e-1d58-4971-abbe-e20c9797afae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1891927-b6d2-4bb1-a15a-4af0eaa633af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [00:03<00:30,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [00:06<00:25,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [00:09<00:21,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [00:13<00:20,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [00:16<00:16,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [00:19<00:12,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [00:22<00:09,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [00:26<00:06,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [00:29<00:03,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [00:32<00:00,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.10635704131036898\n",
      "Population bps:  0.033278320474483106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23934aa8-3590-4d23-be42-29bb1734af7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86120ece-8b8a-4adf-8f0a-f28c3e712e16",
   "metadata": {},
   "source": [
    "#### Intra-Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c42843-05fc-49ce-9e2e-9c87c913a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'intra_region'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cca21da6-1d6d-4104-8138-b33881b4043a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  temporal\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_temporal'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4b2ce85-05cf-4adb-b291-11f9d0d1204b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|██████▉                                                              | 1/10 [04:50<43:32, 290.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|█████████████▊                                                       | 2/10 [05:45<20:16, 152.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|████████████████████▋                                                | 3/10 [06:47<12:55, 110.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|███████████████████████████▌                                         | 4/10 [08:31<10:49, 108.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|███████████████████████████████████                                   | 5/10 [09:05<06:47, 81.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████████████████████████████████████████                            | 6/10 [09:09<03:40, 55.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|█████████████████████████████████████████████████                     | 7/10 [09:50<02:31, 50.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████████████████████████████████████████████████████              | 8/10 [10:13<01:23, 41.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|███████████████████████████████████████████████████████████████       | 9/10 [10:14<00:29, 29.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [10:25<00:00, 62.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: -0.10239252156202386\n",
      "Population bps:  -0.02953215137772669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1442c6-aead-47f2-95c8-58960355f3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c10676f-5a3b-4107-8d86-48c2d5e12695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  temporal\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_temporal'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d97811b5-abce-4e96-bea0-30be02adb8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|██████▉                                                              | 1/10 [04:50<43:37, 290.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|█████████████▊                                                       | 2/10 [05:45<20:16, 152.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|████████████████████▋                                                | 3/10 [06:46<12:53, 110.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|███████████████████████████▌                                         | 4/10 [08:29<10:45, 107.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|███████████████████████████████████                                   | 5/10 [09:03<06:44, 81.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████████████████████████████████████████                            | 6/10 [09:07<03:38, 54.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|█████████████████████████████████████████████████                     | 7/10 [09:48<02:30, 50.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████████████████████████████████████████████████████              | 8/10 [10:10<01:22, 41.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|███████████████████████████████████████████████████████████████       | 9/10 [10:11<00:28, 28.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [10:22<00:00, 62.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: -0.09306077172878184\n",
      "Population bps:  -0.09650128196587167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db020c9-39e9-4d36-901d-438cf579d042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0eebb10-9439-42d5-9f59-b8e802a5b9a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9919f2c8-98bb-4651-a5fe-e3993b083d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|                                                                              | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|██████▉                                                              | 1/10 [04:51<43:45, 291.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|█████████████▊                                                       | 2/10 [05:47<20:22, 152.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|████████████████████▋                                                | 3/10 [06:48<12:58, 111.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|███████████████████████████▌                                         | 4/10 [08:32<10:50, 108.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|███████████████████████████████████                                   | 5/10 [09:07<06:48, 81.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████████████████████████████████████████                            | 6/10 [09:10<03:40, 55.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|█████████████████████████████████████████████████                     | 7/10 [09:52<02:31, 50.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████████████████████████████████████████████████████              | 8/10 [10:14<01:23, 41.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|███████████████████████████████████████████████████████████████       | 9/10 [10:16<00:29, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|█████████████████████████████████████████████████████████████████████| 10/10 [10:26<00:00, 62.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.44763587455676185\n",
      "Population bps:  0.3412413330120953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484dac9-3703-4393-8372-cc8dc374533a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a724c7ac-3ea9-40b4-9301-4d298f9e2687",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/yzhang39/IBL_foundation_model/src/utils/dataset_utils.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  all_datasets = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b806fff-271c-456c-b228-c7736360222d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [04:47<43:05, 287.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [05:41<20:01, 150.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [06:41<12:43, 109.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [08:23<10:37, 106.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [08:57<06:40, 80.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [09:00<03:36, 54.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [09:41<02:28, 49.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [10:03<01:21, 40.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [10:04<00:28, 28.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [10:14<00:00, 61.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.2161721686900901\n",
      "Population bps:  0.08208921644043589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3db405-26f5-421d-8115-27aeb8441783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7532f90d-02f0-44f9-86fd-055a50cfc86a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.6\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "339d61e5-4a00-45f0-968a-9789fa107c22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [04:47<43:08, 287.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [05:41<20:03, 150.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [06:42<12:44, 109.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [08:24<10:37, 106.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [08:57<06:40, 80.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [09:01<03:36, 54.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [09:41<02:28, 49.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [10:03<01:21, 40.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [10:04<00:28, 28.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [10:15<00:00, 61.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.5504777772095283\n",
      "Population bps:  0.3968748183589806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6cabf-3d98-4be2-8f28-14e8deb7400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e750cc78-e0fb-4bfb-ae3c-117157959ff7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9a69e96-9d97-4340-9806-fcee046c8e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  10%|█         | 1/10 [04:47<43:06, 287.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DG-mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  20%|██        | 2/10 [05:41<20:03, 150.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  30%|███       | 3/10 [06:42<12:44, 109.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  40%|████      | 4/10 [08:24<10:38, 106.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  50%|█████     | 5/10 [08:57<06:40, 80.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  60%|██████    | 6/10 [09:01<03:36, 54.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  70%|███████   | 7/10 [09:42<02:29, 49.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  80%|████████  | 8/10 [10:04<01:21, 40.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiber tracts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region:  90%|█████████ | 9/10 [10:05<00:28, 28.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "region: 100%|██████████| 10/10 [10:15<00:00, 61.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.23328750545461416\n",
      "Population bps:  0.09737429814839832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': ['all'],\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20d6fa-0c71-4f07-8af6-5d538d1e4d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96f39849-8636-479b-bd97-7c0c748e220d",
   "metadata": {},
   "source": [
    "#### Co-Smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51c56b6-9137-4233-bda2-6e48ba5cffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'per_neuron'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7075954a-0282-44ac-b410-bcaa41f360cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/u/yzhang39/IBL_foundation_model/src/utils/dataset_utils.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  all_datasets = list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  temporal\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_temporal'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90426a0e-31cd-4ba6-9f17-dec1c605a2c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 531/531 [10:35<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.4799846162247472\n",
      "Population bps:  0.3360382128885546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19170026-6772-449b-ad38-d3cb7f89a295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54d55f7-d15d-41f1-99b7-3e08b83a60cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  temporal\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_temporal'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f53db6cb-64c9-4176-aa30-5b8167ab1632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 531/531 [10:35<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.055388193960706864\n",
      "Population bps:  -0.026114232667973306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b6446-2859-4cc4-a0f4-33a01018b881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cb4a559-1ecc-48a9-8e06-4a1df3ab58a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16138399-7947-4ab4-969a-433be15087a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 531/531 [10:34<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.55913533949669\n",
      "Population bps:  0.400619783577181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b39cf-8464-46ba-a46b-e732c8db2a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa255604-1110-4a95-868e-36e55112c3d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd3c38b4-291d-422e-9831-54bd29dad4a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 531/531 [10:34<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.2311739281707012\n",
      "Population bps:  0.10806386906386051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed3a64-d945-46f3-ae02-45f44f926bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d51d09a-d998-47f2-8ca3-ce2f7afd0b12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.6\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26559f1d-a7ea-4e87-a076-e7227a429c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/531 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.5803542944536544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eab38018-a1d3-4975-9765-1d37b4c6f500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 531/531 [10:32<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.6151437782047022\n",
      "Population bps:  0.43121102561675223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882fc4e-885c-4cc9-9d49-2d0a290af8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3046486a-255f-45c0-9b36-5607f9a33bc3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ea18f6-23d2-48db-a682-49e687410e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/531 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.29362305611300854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80cb82c1-210c-4dc5-aaaa-c8e52072adcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 531/531 [10:26<00:00,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.2149321581471123\n",
      "Population bps:  0.1171720487557235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': None,\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497921b-a004-4960-9c5e-9c663ac0cd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52d9c61d-4514-44e4-99a0-d231b198295f",
   "metadata": {},
   "source": [
    "#### Forward-Predicition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d456b1c9-e85e-4af9-af73-3d69a61fd67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'forward_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1c49c844-c3ee-4da4-9044-23258eab7037",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_name = 'mask_temporal'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f596358d-9c70-4bca-a6df-757bfe2cbfda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "co-bps: 100%|██████████| 531/531 [00:00<00:00, 37675.30it/s]\n",
      "R2: 100%|██████████| 531/531 [00:00<00:00, 3793.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.31155108094541134\n",
      "Population bps:  0.2233862014138918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': list(range(90, 100)),\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f688e-032d-4301-8ba0-a1e05d3e4cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "788c692f-91cb-47e1-b20f-bc9d67cff9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  temporal\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_temporal'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dcabe9a9-259e-44a1-931b-78bc93d34e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "co-bps: 100%|██████████| 531/531 [00:00<00:00, 38107.85it/s]\n",
      "R2: 100%|██████████| 531/531 [00:00<00:00, 3256.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: -0.13744368615400404\n",
      "Population bps:  -0.1117510373624553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': list(range(90, 100)),\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6191f8-a0bc-4b0a-bab1-b4479c95b919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f558d84-4f18-47ea-a92b-7520d8a4acc9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9cd0187-2974-49e3-a985-c0118efcc779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "co-bps: 100%|██████████| 531/531 [00:00<00:00, 35531.34it/s]\n",
      "R2: 100%|██████████| 531/531 [00:00<00:00, 3278.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.2929018679854785\n",
      "Population bps:  0.20494564114693478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': list(range(90, 100)),\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee88bd4-2a2b-44ad-8ba9-e8578a35554f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c196a9a-77ed-4eb9-a2f8-4c6cb3ab4044",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "128cdd38-c6b2-4fe4-8918-30270bb3cea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "co-bps: 100%|██████████| 531/531 [00:00<00:00, 35304.36it/s]\n",
      "R2: 100%|██████████| 531/531 [00:00<00:00, 3860.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: -0.01513936966217645\n",
      "Population bps:  -0.017191849327235005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_single/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': list(range(90, 100)),\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403eb708-c627-4b45-8f25-559830995340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7905416c-960d-4d70-8869-9e1278c7ca67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.6\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = False\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}   \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d50ba845-d60b-4795-b1fa-4ac2c1bfef8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "co-bps: 100%|██████████| 531/531 [00:00<00:00, 37817.32it/s]\n",
      "R2: 100%|██████████| 531/531 [00:00<00:00, 3810.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.31155108094541134\n",
      "Population bps:  0.2233862014138918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': list(range(90, 100)),\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d147b4-c48d-475b-8728-788d8359d6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c6181d02-968a-4f1c-aee9-755267b2b5f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed set to 42\n",
      "Total session-wise datasets found:  240\n",
      "Loading train dataset sessions for predefined train/val/test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session eid used:  {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}\n",
      "Total number of session:  1\n",
      "Train dataset size:  384\n",
      "Val dataset size:  48\n",
      "Test dataset size:  112\n",
      "encoder max space length: 531\n",
      "{'num_neurons': [531], 'num_sessions': 1, 'eids': {'03d9a098-07bf-4765-88b7-85f8d8f620cc'}, 'tokenize_binary_mask': False, 'max_space_length': 531}\n",
      "(eval) masking mode:  all\n",
      "(eval) masking ratio:  0.3\n",
      "(eval) masking active:  False\n",
      "spike data shape: torch.Size([114, 100, 531])\n"
     ]
    }
   ],
   "source": [
    "mask_name = 'mask_all'\n",
    "nonrandomized = True\n",
    "\n",
    "# Configuration\n",
    "configs = {\n",
    "    'model_config': model_config,\n",
    "    'model_path': f'{base_path}/results/finetune/num_session_1/model_{model_name}/method_ssl/{mask_name}/stitch_False/{eid}/{best_ckpt_path}',\n",
    "    'trainer_config': f'src/configs/finetune_sessions_trainer.yaml',\n",
    "    'dataset_path': None, \n",
    "    'test_size': 0.2,\n",
    "    'seed': 42,\n",
    "    'mask_name': mask_name,\n",
    "    'eid': eid,\n",
    "    'stitching': False,\n",
    "    'num_sessions': 1,\n",
    "    'tokenize_binary_mask': tokenize_binary_mask,\n",
    "    'nonrandomized': nonrandomized,\n",
    "}  \n",
    "# load your model and dataloader\n",
    "model, accelerator, dataset, dataloader = load_model_data_local(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "936af405-4865-452f-bb05-f9d8324a33cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "co-bps: 100%|██████████| 531/531 [00:00<00:00, 35182.22it/s]\n",
      "R2: 100%|██████████| 531/531 [00:00<00:00, 5476.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-neuron bps: 0.04542824188189382\n",
      "Population bps:  0.02667111786592492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "co_smoothing_configs = {\n",
    "    'subtract': 'task',\n",
    "    'onset_alignment': [40],\n",
    "    'method_name': mask_name,\n",
    "    'save_path': f'{work_dir}/notebooks/plots/{model_name}/{eid[:4]}_multi/nonrandomized_{nonrandomized}/{mask_name}/{task}',\n",
    "    'mode': task,\n",
    "    'n_time_steps': 100,    \n",
    "    'held_out_list': list(range(90, 100)),\n",
    "    'is_aligned': True,\n",
    "    'target_regions': None,\n",
    "    'n_jobs': 1\n",
    "}\n",
    "gt_result_list, pred_result_list, behavior_set, y_list, y_pred_list, y_residual_list = co_smoothing_eval(\n",
    "    model, accelerator, dataloader, dataset, save_plot=True, **co_smoothing_configs\n",
    ")\n",
    "pred = np.stack(pred_result_list, -1).squeeze()\n",
    "gt = np.stack(gt_result_list, -1).squeeze()\n",
    "print('Population bps: ', bits_per_spike(pred, gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ebf37-72f5-4baa-aa4a-b5b2bc20a2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
