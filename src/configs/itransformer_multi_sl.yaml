model_class: iTransformer


encoder:
  from_pt: null

  masker: 
    force_active: True
    mode: temporal        # masking mode
    ratio: 0.0          # ratio of data to predict
    zero_ratio: 1.0     # of the data to predict, ratio of zeroed out
    random_ratio: 1.0   # of the not zeroed, ratio of randomly replaced
    expand_prob: 0.0    # probability of expanding the mask in ``temporal`` mode
    max_timespan: 10     # max span of mask if expanded
    channels: null        # it's not used?
    mask_regions: ['all']       # brain regions to mask in ``inter-region`` mode
    target_regions: ['all']    # brain regions to predict in ``intra-region`` mode
    n_mask_regions: 1       # number of regions to choose from the list of mask_regions or target_regions
    causal_zero: false       # only for iTransformer causal mode
    timesteps: null          # for forward prediction

  embedder:
    bias: true
    activation: relu
    dropout: 0.2
    max_n_bins: 100

  hidden_size: 512
  activation: relu          
  bias: true
  dropout: 0.4
  n_heads: 8
  n_layers: 5
  max_n_channels: 0        # 0 to skip channel embeddings
  embed_region: true
  neuron_regions: null       # not used in iTransformer_multi
  attn_mode: all       # all, inter-region, inter-region-switch, intra-region, mix_sequence, mix_multihead, mix_sample
  attn_mix_n: [2, 2, 4]        # inter-region, intra-region, all, should add up to n_heads (mix_multihead)
  attn_mix_seq: ['intra-region', 'intra-region','inter-region', 'intra-region', 'intra-region'] # sequence of residual block (mix_sequence)
  attn_mix_ratio: [0.25, 0.25, 0.5] # inter : intra : all, should add up to 1 (mix_sample)

decoder:
  from_pt: null
  mlp_decoder: true  # or it will be a linear decoder
  activation: relu
  use_cls: true
