model_class: MultiModal

use_session: false

masker:
  force_active: true         
  mode: temporal              # masking mode
  ratio: 0.3                  # ratio of data to predict
  zero_ratio: 1.0             # of the data to predict, ratio of zeroed out
  random_ratio: 1.0           # of the not zeroed, ratio of randomly replaced
  expand_prob: 0.0            # probability of expanding the mask in ``temporal`` mode
  max_timespan: 1             # max span of mask if expanded
  channels: null              # neurons to mask in ``co-smooth`` mode
  timesteps: null             # time steps to mask in ``forward-pred`` mode
  mask_regions: ['all']       # brain regions to mask in ``inter-region`` mode
  target_regions: ['all']     # brain regions to predict in ``intra-region`` mode
  n_mask_regions: 1           # number of regions to choose from the list of mask_regions or target_regions
  causal_zero: true           # only for iTransformer causal mode

context:
    forward: -1
    backward: -1

encoder:
  from_pt: null

  embedder:
    n_modality: 2
    n_channels: 668       # number of neurons recorded 
    max_F: 100           # max feature len in timesteps
    mult: 2               # embedding multiplier. hiddden_sizd = n_channels * mult
    pos: true             # embed position 
    act: softsign         # activation for the embedding layers
    scale: 1              # scale the embedding multiplying by this number
    bias: true            # use bias in the embedding layer
    dropout: 0.2          # dropout in embedding layer

  transformer:
    n_layers: 5           # number of transformer layers
    hidden_size: 256     # hidden space of the transformer
    use_scalenorm: false  # use scalenorm  instead of layernorm
    n_heads: 8            # number of attentiomn heads
    attention_bias: true  # learn bias in the attention layers
    act: gelu             # activiation function in mlp layers
    inter_size: 512      # intermediate dimension in the mlp layers
    mlp_bias: true        # learn bias in the mlp layers
    dropout: 0.4          # dropout in transformer layers
    fixup_init: true      # modify weight initialization


decoder:
  from_pt: null

  decoder_sep_mask: false

  embedder:
    n_modality: 2
    n_channels: 668       # number of neurons recorded 
    max_F: 100           # max feature len in timesteps
    mult: 2               # embedding multiplier. hiddden_sizd = n_channels * mult
    pos: true             # embed position 
    act: softsign         # activation for the embedding layers
    scale: 1              # scale the embedding multiplying by this number
    bias: true            # use bias in the embedding layer
    dropout: 0.2          # dropout in embedding layer

  transformer:
    n_layers: 5           # number of transformer layers
    hidden_size: 256     # hidden space of the transformer
    use_scalenorm: false  # use scalenorm  instead of layernorm
    n_heads: 8            # number of attentiomn heads
    attention_bias: true  # learn bias in the attention layers
    act: gelu             # activiation function in mlp layers
    inter_size: 512      # intermediate dimension in the mlp layers
    mlp_bias: true        # learn bias in the mlp layers
    dropout: 0.4          # dropout in transformer layers
    fixup_init: true      # modify weight initialization
    
